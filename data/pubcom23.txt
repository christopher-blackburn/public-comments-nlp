 Metzenbaum submission to the Advisory Committee on Data for Evidence Building, Feb. 9, 2021 1   Comments submitted to Advisory Committee on Data for Evidence Building on   Promoting the Use of Data for Evidence Building   and for Evidence Sharing and Evidence Use        from  Shelley H. Metzenbaum, Ph.D.   Former OMB Associate Director for Performance and Personnel Management   February 9, 2021   This comment is informed by my work on a report forthcoming from the IBM Center for the Business of Government on improving grant outcomes, operational quality, and transparency.      Metzenbaum submission to the Advisory Committee on Data for Evidence Building, Feb. 9, 2021 2   OVERVIEW  This section, the Overview, provides overview answers to questions 1 through 3 and questions 9 through 10. It does not respond to questions 4 through 7.   The next section, Discussion and Examples, elaborates on questions 1 through 3.   1. What are the main challenges faced by national, state/provincial, or local governments that are  trying to build a basis for evidence-based policy?  Briefly describe the bottlenecks and pain-points the face in the evidence-based decision-making process.   Efforts by states, localities, tribes, territories, regions, and community-based non-profits to use evidence (and contribute to building it) have been greatly complicated over the years by a number of bottlenecks, pain points, misconceptions, and other problems that leave many of the lessons of experience and of well-designed trials unappreciated and unshared. These problems include:   A. Asking the wrong questions B. Too few asking the right questions and using the full scope of evidence to inform action C. Siloed implementation organized around function and program, not outcomes and beneficiaries D. Insufficient attention to sharing evidence successfully with key audiences  E. Oversight infrastructure that overwhelms insight generation and insight sharing  F. Incentive systems that ignore evidence about motivational mechanisms  G. Weak systems for learning and building knowledge across programs about what works well and  what works less well for building, sharing, and using evidence  These problems are explained and discussed in greater detail in the Discussion and Examples section below.    Please note that while the December 15, 2020 Federal Register notice asked about challenges facing governments trying to build a basis for evidence-based policy, I urge you to consider the evidence needs of non-profit organizations, as well. In a number of areas, such as boosting knowledge through R & D and reducing poverty and discrimination, federal policy makers have intentionally opted with sound policy reason for the federal government to work directly with non-governmental organizations rather than through other governments as intermediaries. I urge the Advisory Committee to consider how to support the evidence needs of and lessons learned from the federal government’s partnership with non-profit organizations as varied as major universities and local community action programs because these NGOs are so critical to continuous learning and improvement in many policy areas.   2. What are examples of high-impact data uses for evidence-based policy making that successfully  effected change, reduced costs, or improved the welfare of citizens?  Governments, non-profits, and the for-profit private sector have long generated, shared, and used data over the years with high impact returns as well as higher return on spending in areas as diverse as agriculture, environmental quality, health, transportation safety, and early childhood. Sadly, while these success stories have occasionally been captured in narrative form over the years, to my knowledge no one has tried to capture lessons from successful and less successful data-using, evidence producing, and evidence-sharing experience in systematic ways to make it easier to learn from experience as well as from well-designed trials across policy areas what worked well, what did not, and why. I notice that the Federal Register notice refers to Round 1. I am not sure what that means, but hope it means you intend this to be the beginning of an iterative learning and improvement effort that makes the federal government more    Metzenbaum submission to the Advisory Committee on Data for Evidence Building, Feb. 9, 2021 3  evidence-based in the way it builds, shares, and uses evidence in addition to informing how federal programs can help their delivery partners to build and use evidence for better outcomes, operational quality, and transparency. In other words, please make the information collection effort you have stared with this Federal Register Request for Comments the beginning of an ongoing effort to build and share evidence about more and less effective ways for the federal government to build, share, and use evidence going forward.   Let me also urge you to start to be specific about different uses and users of evidence. Doing so may relieve some of the confusion and debate currently surrounding evidence-based policy. Let me suggest that we need evidence to:   • Inform where to focus;  • Find ways to improve – search for what works, for what works better, and for situational  differences that affect effectiveness; and • Increase uptake of better practices and reduced use of less good ones. • On occasion, inform choice among providers and products, and  • Support referrals and client and outcomes-based integration across programs, providers, and  services.  Let me also suggest that the Advisory Committee call on federal agencies, especially but not only grant-giving agencies and those involved in cooperative agreements, to treat those who work on the frontline and those who support them as priority users for federal data, analyses, findings of well-designed trials, and other evidence. Doing so will require careful attention to where, when, and how evidence is communicated and to making sure it is communicated successfully to make sure that target users can easily find the evidence they need when they need it in a format they can easily access, understand, and apply appropriately. It will require building and sharing evidence about the effectiveness of evidence communication efforts and how the effectiveness of different approaches may vary for different evidence users. Many of the evidence repositories that share the findings of well-designed trials from non-medical RCTs currently communicate evidence in ways that make the findings hard to access and, once accessed, hard to interpret accurately and apply appropriately. Moreover, much of what gets reported in these evidence repositories focuses on whether or not a particular treatment or product was effective on average rather than also providing more actionable information about differential effects (for whom did a treatment work and for whom did it not) and comparative effectiveness. Progress has been made in the design of some evidence repositories in recent years, but much more is needed.   Valuable lessons can be learned about useful ways to share data, analyses, and the findings of well-designed trials from across the federal government and beyond the U.S. both about what works well and what works less well for different audiences. At the same time, well-designed trials could be undertaken to find even better ways to communicate data, analyses, and other evidence. The federal government should create and support a network of evidence repository managers to function as a continuous-learning-and-improvement community, learning across programs and collaborating to get feedback from users while also testing to find better communication methods.   I provide examples of high-impact data uses, as well as less impactful uses, in the Discussion and Examples section below, and also discuss data users in that context.   3. Which frameworks, policies, practices, or methods show promise overcoming challenges  experience by governments in their evidence-building?     Metzenbaum submission to the Advisory Committee on Data for Evidence Building, Feb. 9, 2021 4  Whether talking about cleaner air and water, safer travel, healthier people, helping children thrive in life, or better agricultural practices, the most promising frameworks, policies, practices, and methods share several characteristics:    • they are clear about the outcomes they seek to improve (although outcome objectives may change over time) and find ways to measure progress, problems, and context to inform where to focus and find ways to do better;  • they communicate analyzed data and the findings of well-designed trials to current and often potential delivery partners in and outside the federal government in appropriately frequent, timely, understandable, easily accessible, and useful ways;  • they embrace routines that discuss data analyses and findings of well-designed trials with those involved in delivery to figure out what has been learned, understand risks and causal factors that need attention, and decide what to do and what to learn next.  • They also communicate to key authorizers, other interested parties, and the public in easily understood and resonant language about outcome goals and why they were chosen, strategies and why they were chosen, progress, problem, lessons learned, and planned next steps both short and long term.     Questions 4-7 regarding the National Secure Data Service. I do not feel expert enough to contribute to this important discussion except to urge you to be very clear about the risks you are trying to prevent, such as identity theft, to figure out how to prevent them.   8(a). What are the most pressing data needs of state and local decision makers and how would making data accessible from federal agencies help meet those needs?    State, local, tribal, territorial, and regional governments, as well as non-governmental organizations that are federal purpose partners through grants or cooperative agreements or other means need data to think and act more intelligently about:   • Where to focus (e.g., whom to serve, where and when to take action) o Detect and understand problems needing attention, their relative import, whom they  affect, where and when they occur, and why they occur o Areas of opportunity  o Context for action (e.g., changes in supply, demand, other changes in the world that may  need attention such as the relative risk of self-driving cars) • Ways to improve  o Find predictive, precursor, or warning indicators of problems and progress, as well as those that can shed light on causal and other pathways that can be influenced.  o Detect progress to be sustained and possibly replicated and/or amplified, relative import, whom affected, where and when happening, and why  o Find positive outliers with replication-worthy practices worth testing in other places to see if they generate similarly better results  o Detect anomalies and outliers to understand why they are happening and whether they point to problems needing attention as well as to better practices worth replicating in other places  o Inform treatment design by better understanding characteristics of the people, places, and organizations to be affected (e.g., is a job training program for single parents new to the workforce, for long-time workers laid off because a plant shut down, or both; is a driving    Metzenbaum submission to the Advisory Committee on Data for Evidence Building, Feb. 9, 2021 5  safety program for new drivers or the subset of older drivers that analyses suggest have more accidents; do unwanted incidents cluster at different times of the day, week, or year; do different population groups have different attitudes toward vaccination.)    o Figure out how products and practices currently used or being considered compare to other products and practices available to meet the same needs, resisting the temptation to favor what already dominates the market that state, local, and non-profits may feel it is safer to use because everyone is using even when it is not as good as alternatives or does not meet needs  o Figure out for whom products and practices work well and for whom they work less well  o Find others dealing with similar circumstances with whom to collaborate to develop  tools to meet similar needs and to test and assess new practices to find better ways to do business.   • Increasing uptake of better practices and reduced use of less good ones • On occasion, informing individual choice, such as where to rent a home to get a school that is a  good fit for a child’s learning needs, live near open space, or find cleaner air for an asthmatic child  • Serving people, communities, places, and economic activities in more intelligent, simple, and cost-effective ways with more useful, interactive information about eligibility requirements and service provider location and quality for referrals and collaboration (e.g., Code for America has done very interesting work on this)  • Relationships (systems, networks, vectors, causal pathways) to understand what affects what and to tap into those relationships to realize better outcomes in fair, equitable, cost-effective ways   8(b). To share data, what guarantees do data owners (or data controllers) need regarding privacy, data stewardship, and retention?  Data Sharing and Stewardship. How the federal government collects and returns data and data analyses as well as findings of well-designed trials to the field makes a big difference in how valuable shared data are likely to be and how enthusiastically data sharers are likely to engage. The federal government needs to return data it collects from the field back to the field, and especially to data suppliers, in a timely manner with value added through analyses in ways that allow states, localities, and others to learn not just from their own experience but also from the experience of others. The federal government should also return data in ways that make it easier for those on the front line to find others like them from whom they can learn and with whom they can collaborate to test and assess to find better and more cost-effective approaches to address different situations. If the federal government collects data but fails to return it to the field with value added through analyses, the field will treat data as a compliance exercise that needs to be completed without worrying about the quality of the data they submit. This tends to result in less valuable data and time wasted collecting and reporting it.   Also, if the federal government or others opt to use data as part of a reward or punishment system or use it to make unfair comparisons instead of requiring data to be collected and reported and then returning data to the field with value added through analyses, the field is likely to view data as a threat rather than a resource. When that is the case, the field tends to fight to dismantle the measurement or the incentive system (e.g., motorcycle helmet laws), advocate successfully for measurement that does not reveal much that is useful for finding ways to improve or where to focus, game the system or manipulate the measures (e.g., education metrics in Dallas and elsewhere), or treat data as a burdensome requirement that needs to get done rather than a way to build knowledge. That is not to say that the federal government should never link data to the promise of a reward or the threat of punishment. When done well, as with the National Ambient Air Quality Standards of the Clean Air Act (CAA), the linkage can be very effective. It is important to note, however, that the CAA linkage to the threat of a large punishment (loss of    Metzenbaum submission to the Advisory Committee on Data for Evidence Building, Feb. 9, 2021 6  transportation funding) only worked well after the U.S. Environmental Protection Agency made assumptions of the model it used to project attainment of air quality standards more transparent and revised those assumptions in close consultation with the states threatened by penalties if projected performance of their planned actions were not projected to attain CAA standards.   The federal government also needs to work with state, local, and non-profit partners to avoid using data in ways that might contribute to unfair or unconstructive bias and in ways that they collect and use biased data. Unbiased data can cause problems for the people whom government wants to benefit when it causes those using data adjust their assumptions downward about what is possible in ways that hurt the performance of the people or places being measured. One example of this has been referred to as the Pygmalion effect. Researchers have found that teacher expectations for an incoming student tend to affect how those teachers teach each student and consequently how they perform, reducing the future performance of those entering a classroom with lower past performance scores.1   Unbiased data can also cause problems for government because, cognitive scientists have found, people not only tend to pay more attention to the negative and sensational but also remember negative and sensational stories longer than positive ones. This bias to the negative and sensational is more of a problem for government than for publicly traded private companies because investors actively search for companies and organizational units with promising stories and high returns, which counterbalances the bias to the negative that government suffers. Those working on data stewardship need to give serious attention to finding ways to offset this negativity bias to reduce the likelihood that stories about government problems will overwhelm stories about government success, triggering a vicious cycle of distrust and unwillingness to fund government that, in turn, makes government less successful over the longer term.   Bias in data collection can also be a serious problem in fields as diverse as medicine, policing, terrorist screening, and early childhood assessments. The bias can arise because of past bias and discrimination in practice, but also from screening and testing tools that did not include a diverse subset of the population used to develop the screens or tests.  Data stewardship requires serious attention to finding, being aware of, and preventing bias as well as preventing biased decisions and actions informed by biased data.     Given the risks of problems using unbiased information as well as the risks of using biased information, I urge you to recommend that all federal agencies make bias, as well as the questions of allowed and unallowed as well as encouraged and discouraged uses of collected data a topic of much deeper inquiry in the future because it is so important. I would also urge you to recommend the creation of a continuous-learning-and-improvement network across the federal government to learn from experience about what and how to share data in ways that motivate continual improvement and allow fair comparisons in ways that shed light on better practices and products but do not tempt measurement manipulation or worse and that avoid unfairness and inequity.   Successful data sharing and stewardship calls for careful attention and mutual agreement not just to privacy, but also to assuring appropriate and inappropriate uses of data as well as figuring out how to encourage some and discourage other uses.      1 http://adigaskell.org/2014/10/24/research-provides-more-evidence-of-the-pygmalion-effect/#:~:text=The%20scores%20from%20the%20teachers,pupils%20had%20higher%20performing%20classes.\         Metzenbaum submission to the Advisory Committee on Data for Evidence Building, Feb. 9, 2021 7  Privacy. Data privacy is important. Government needs to prevent the sharing of personally identifiable information used to steal another person’s identity, unfairly embarrass them, or reveal personal information they do not want revealed. It also needs to prevent the release of confidential business information that creates unfair economic competition. At the same time, access to more granular data with details about the finest unit of analysis possible, whether about a person or an event or both, combined with frequent and timely data noting key characteristics about the person or event such as the operator/provider, equipment/products used, location, and time of the event or the measurement makes data far more useful for figuring out where to focus and for learning from experience what worked well and what did not to inform what to do or buy next. Progress needs to be made to de-identify personally identifiable and confidential business information while still being able to mine the masses of data government collects to learn more about, for example, whether a particular curriculum or job training program helped more people in certain categories than did other programs and which kinds of people they helped.   Retention. Data retention policies should encourage retention in ways that make it possible and easier to learn from experience, such as by comparing long and shorter-term trend data across locations together with snapshot comparisons for the most recent timeframes. This often enables not just more fair comparisons but identification of those that found ways to improve and those encountering new problems that may need help they did not previously need.   Data retention policies should also retain information about cross-agency, agency, and program goals and more specific objectives (required by the Government Performance and Results Act of 2010 and its predecessor and by program-specific enabling laws), as well as data, analyses, and other studies used in the past to inform where to focus and decide strategies and next steps. This will make it more possible to learn from experience both what worked well and what did not and decide future action. It is currently difficult if not impossible to find past federal government goals and experience that used to be visible on ExpectMore.gov and earlier iterations of Performance.gov except by getting lucky on the Wayback Machine. Snippets of the information can be found on Presidential archives, but not in a format that makes the information useful for learning from experience. In the age of big data and following the passage of the Foundations of Evidence Act, it is time to start archiving this information in more useful ways and relating it to relevant government-wide outcome objectives (perhaps starting with budget functions and sub-functions as those outcome categories but evolving from there), data bases, evidence repositories, and learning agendas.   9. What are the key problems and use cases where collaborative work between federal, state, and local authorities’ data analyses can inform decisions?  What are key decision support tools? How would greater communication about data and tools benefit expended evidence building?  The federal government depends on others to advance its outcome objectives for almost all domestic and many international problems and opportunities. This is true whether the objectives are to deter harmful incidents (e.g., fires, crime, accidents, deaths) or encourage beneficial ones (e.g., restoring endangered species, healthy births), improve conditions (e.g., air and water quality, ecosystem and economic system health), attain and sustain specific performance levels (e.g., drinking water and workplace standards), or realize discrete accomplishments (e.g., knowledge building through research.) In all of these situations, the way the federal government collects, shares, and uses data, data analyses, and the findings of well-designed trials can be enormously helpful to local decision-making, action, continuous learning, and continuous improvement.   At the same time, the federal government’s data handling decisions can be problematic. Insufficient attention to how to collect and return information to help front line and other current and potential    Metzenbaum submission to the Advisory Committee on Data for Evidence Building, Feb. 9, 2021 8  purpose partners make better decisions for better results can be a problem. So can inappropriate or problematic use of collected information, such as to reward or punish or to compare unfairly. Too often, federal agencies handle the information they collect in ways that distracts attention away from improving outcomes, operational quality (e.g. service quality, cost effectiveness, risk), and transparency for multiple purposes, unfortunately focusing it instead on staying out of trouble and being in compliance. That is not to say that attention to risks is not important. Attention to actual and potential operational problems is very important, including attention to the risks of unwanted side effects such as police abuse and measurement manipulation. Risk and problem information, too, needs to be collected, analyzed, and shared with the field in ways that help them understand and manage risk wisely.  In short, the federal government takes on few “key problems” and “use cases” where a collaborative approach to states, locals, and non-profits would not lead to better results. Exceptions might be when the federal government pursues major discrete objectives such as landing on the moon or Mars and aspects of national defense. To collaborate successfully using data and well-designed trials, the federal government needs to communicate frequently with its current and potential purpose partners to sort out what and how it can help, including how it can best collect, organize, analyze, and share data, data analyses, evaluation findings, and other studies to be more useful. It also needs to communicate frequently and effectively to identify synergistic roles the federal government is better positioned to play than states, localities, and non-profits acting on their own, such as developing shared tools and knowledge.    (See the response to question 10 for an answer to your question about key decision support tools likely to be helpful.)  10. What basic public data services are essential for a data service to address existing capacity gaps and needs? What infrastructure or incentives can the federal government create that locals and states cannot?  First, public data services need to identify priority users and apply user-centered design principles (and perhaps other customer service practices) to make sure users are aware of the data service and other evidence resources that are available to them, can find and access them easily (e.g., no paywalls), can understand how to apply the research findings appropriately, and can understand how to use data system tools such as filters, report generators, and visualization generators. Purpose partners should be treated as priority users. These include those on the front line, including state, local, tribal, and non-profit grantees, but also purpose partners in other parts of the federal government and those who support the front line with training and technical assistance.   Second, the federal government should make it easy to organize and integrate data system information around outcomes, populations served, and providers. It should do this in a way that makes it easier to find others working on the same or related objectives both to learn from and to collaborate with them.    Third, the federal government can play a synergistic role in numerous ways helping their front-line purpose partners make more sense of data and research findings. These include:   • Collecting data in ways that make it more actionable. Federal agencies should work with states, localities, non-profits, and other current and potential purpose partners to find ways to characterize the information they collect to make it more useful for figuring out where to focus and find ways to improve. This may involve time stamping and geo-coding data when collected, but also noting operator/provider and equipment/product characteristics associated with what is counted. (The Haddon Matrix used by the National Highway Traffic Safety Administration is one example of a good way to count and characterize incidents.) Other data collection features to consider to improve data relevance and insight-generating value are the timing and temporal and    Metzenbaum submission to the Advisory Committee on Data for Evidence Building, Feb. 9, 2021 9  spatial frequency of data collection. Teachers lament that end-of-year test scores are of little value to the prior year teacher, while sharing those test results with next year teachers runs the risk described earlier of triggering the unfortunate Pygmalion effect. Would earlier testing be more useful for helping students improve? Monthly water quality data collected by volunteers at 37 points along the 80-mile stretch of the Charles River in Massachusetts revealed to EPA previously unknown water polluters who were illegally discharging into storm sewers that flowed directly into the river rather than flowing, at a permitted level, into the sanitary sewer system for treatment before release into receiving waters. The geographic and temporal frequency of the data made it far more actionable, suggesting where previously unknown problems were happening and also making it possible to see whether or not and how much progress was being made after actions were taken. The river went from being swimmable 19 percent of the time when measurement started in 1995 to being swimmable 51 percent of the time in 1998 after fresh, frequent water quality measurement revealed illicit hook-ups to storm sewers that were then removed. Similar attention to detailed data to distinguish freshman students on track to graduate and those at risk in need of more tailored attention may have increased graduation rates in the Chicago Public Schools significantly.2  • Analyzing and visualizing data. The federal government can offer states, locals, and non-profit purpose partners easy-to-use filters, report generators, and visualization tools to help them figure out where to focus and find ways to improve. The Data Design Initiative of the National Head Start Association is currently working with students at Carnegie Mellon University to develop a tool to make it easier for local Head Start programs to pull and analyze data from the U.S. Census for their mandated but also useful, one hopes, annual needs assessments, for example. The federal government could develop tools such as this to facilitate the sorts of analyses that will help states, locals, non-profits, and even local business networks figure out where to focus and think in more informed ways about what to do next. The federal government could also offer visualization tools such as those at demonstrated by Hans Rosling in a TED talk urging his audient to “Let my data set change your mindset,”3 visualizing data to urge policy makers to focus in places other than where popular opinion was suggesting they should focus. Rosling’s presentation also suggests ways federal agencies might make it easier to see changes over time for more than one variable and more than one location. To support states and localities in their efforts to meet a new federal requirement that federally-supported state and local educational agencies (school districts) produce report cards for parents and the public, the U.S. Education Department held listening sessions with parents and other stakeholders to gather feedback on the format and accessibility of report card information, sponsored a report card design challenge, convened state report card communities of practice (CoP), brought in subject-matter experts to help states and districts learn how to add more data to report cards and how to communicate complex data to external and internal stakeholders more successfully, and created a report card resource library that provides examples of how states are designing and communicating about their report cards. It also helped states with common, although not high volume, special communication needs, such as for Arab American, Native Hawaiian, or Pacific Islander English language learners.4  • Evidence repositories organized around outcomes (problems and opportunities) and populations served. The federal government would help state, local, non-profits, and others in and outside the federal government make more sense of research findings by organizing evidence   2 https://www.educationnext.org/straight-conversation-emily-krone-phillips-chicagos-freshman-ontrack/  3 https://www.ted.com/talks/hans_rosling_let_my_dataset_change_your_mindset/transcript?language=en 4 Education Department, “U.S. Department of Education FY 2019 Annual Performance Report and FY 2021 Annual Performance Plan,” U.S. Department of Education. https://www2.ed.gov/about/reports/annual/2021plan/fy2019apr-fy2021app-report.pdf (p.30-31)    Metzenbaum submission to the Advisory Committee on Data for Evidence Building, Feb. 9, 2021 10  repositories around outcomes and populations served, by reporting not just average findings but also findings about differential impacts of what was studied (e.g., for whom did something work and for whom did it not), and comparative effectiveness. It could show links to other relevant research findings, as PubMed sometimes does. It could provide information in a format that answers the kinds of questions front line workers and managers in the field as well as beneficiaries are likely to have. Understanding what those questions are would require frequent conversations with and feedback from the front line. It would also require translating research findings from peer-reviewed journals and other sources to language the lay public can understand and accurately interpret and apply, confirming that understanding with user feedback. Some of the better medical journals reportedly pay skilled writers to do that translation which, perhaps surprisingly, makes some of the medical research more understandable to potential beneficiaries than other social science findings. Finally, these evidence repositories should explore ways to integrate information from the field about postive outliers, places showing significant progress compared to peers or prior results, such as Chicago public schools, community colleges participating in networked improvement communities, and the clean-up of the Charles River in Massachusetts.  • Learning agendas organized around outcomes and populations served. The Foundations of Evidence Act requires agencies to provide and annually update what many colloquially refer to as a learning agenda, identifying knowledge gaps and sorting out priorities for filling them. (At least, I think and hope that is what the law requires.) Learning agendas, like evidence repositories, should be organized around outcome objectives and problems to be solved, not by agency or program. They should also be organized around beneficiaries in some way, as well. These learning agendas should be dynamic (continually updating) and inclusive (inviting everyone to identify knowledge gaps.) They should support discussion and debate about the relative import of knowledge gaps to be filled and their sequential dependencies. These outcomes-focused learning agendas should also make clear which knowledge gaps the federal government and others are already working to or have plans to fill. The hope is that this will help other interested researchers and funders consider in a more informed ways the knowledge gaps they might want to fill. Outcome-focused learning agendas should link to relevant data bases, relevant analyses, and relevant evidence and evidence repositories. They should also link to relevant federal cross-agency, agency, and program outcome goals and objectives, together with descriptions of strategies and why goals, specific objectives, and strategies were chosen, lessons learned, and planned next steps. The format used for HealthyPeople.gov 2020 includes much of this kind of information (although just national key indicators and not federal government’s goals, objectives, and strategies) that suggests a useful model upon which to build especially if the platform can be used to support local discussions and brainstorming about what to do next, not just what to report next. (HealthyPeople.gov had some elements of this.) ClinicalTrials.gov may also suggests a method for reporting on current research underway as well as planned research that other programs can use.   • Strategy maps and strategy mapping tools. Strategy mapping can be a valuable way to sort out and communicate how all the pieces of an outcomes-improving puzzle fit together, but the effort to create a strategy map can also easily morph into a rigid compliance exercise where everyone feels compelled to fill out the blanks or do what is in the map even when the world has changed rather than using it as a tool to organize conversations about what to do next and who will do it, informed by evidence. Online mind mapping tools that support brainstorming about relationships and strategies may be a helpful way to update strategy mapping and do it in more innovative, agile ways. Federal agencies should explore the use of mind mapping, complemented by ready access to relevant analyses and research findings, to support continual brainstorming and decision making about who will do what next, informed by relevant evidence. They should explore the use of strategy mapping tools not just for national decision-making, but also to support sub-national decisions and actions to improve outcomes.     Metzenbaum submission to the Advisory Committee on Data for Evidence Building, Feb. 9, 2021 11  • Collaboration and learning platforms. Technologies that support collaboration and learning across time and location existed long before Covid-19, but the pandemic led to explosive developments in the capacity, functionality, and sophistication of these communication platforms.  Federal agencies, working individually and with each other, are uniquely well positioned to further develop these communication platforms to support collaboration and continuous learning and improvement within and across communities.   • Problem-solving (and opportunity advancing) routines. The federal government can establish routines to support the field and its purpose partners in looking at data to find pattern similarities and differences, outliers, and anomalies to ask questions about them to figure out where to focus, find ways to improve, and increase uptake of better practices and reduced use of less good ones. It can also play a critical role finding and telling the story about what is known about problems (and opportunities), causal factors affecting them, progress, problems, and lessons learned because the federal government has size and scope that others lack. It is especially well-positioned to do this work looking at experience across the country in a way that states, localities, and non-profits cannot do and to fund well-designed trials to find ways to improve and to communicate better. When the federal government does this and does it well, supporting the field and paying attention to its needs and its understanding, it aligns incentives with continuous learning and improvement. When, instead, it sends out monitors and auditors and inspectors general that collect data but don’t analyze it and share those analyses in ways that help everyone improve, it sends a strong message that compliance and problem avoidance is the order of the day. In a world where trial and error marks the path to improvement provided people at all levels of government and their purpose partners take the time to learn from both, the lack of continuous learning and improvement routines hard wires problematic incentives.      Metzenbaum submission to the Advisory Committee on Data for Evidence Building, Feb. 9, 2021 12  Discussion and Examples   This section elaborates on questions 1 through 3.  1. What are the main challenges faced by national, state/provincial, or local governments that are  trying to build a basis for evidence-based policy?  Briefly describe the bottlenecks and pain-points the face in the evidence-based decision-making process.   Efforts by states, localities, tribes, territories, regions, and community-based non-profits to use evidence (and contribute to building it) have been greatly complicated over the years by a number of bottlenecks, pain points, misconceptions, and other problems that leave many of the lessons of experience and of well-designed trials unappreciated and unshared. These problems include:   A. Asking the wrong questions. For many years, some evidence advocates have argued that the federal government should use evidence to find and fund what works, defunding what does not. That question oversimplifies how evidence can and should be used. Funding what works and defunding what does not ignores the reality that most government grant and other programs were created to reduce problems or advance opportunities that remain important even when the practices a program is using are not working. A grant should not be defunded if an evaluation of the program as a whole finds the “program” ineffective provided the program is actively and intelligently searching for better practices to improve outcomes and is successfully promoting adoption of the better practices and discouraging use of harmful or less good practices, as well as providing information to inform debate about the relative import of the problems and opportunities the grant program is designed to address. As noted in the Overview section, federal agencies should build, share, and use evidence to inform their own and their state, local, non-profit, and other purpose partners to:   • Inform where to focus;  • Find ways to improve – search for what works, for what works better, and for situational  differences that affect effectiveness; and • Increase uptake of better practices and reduced use of less good ones. • On occasion, inform choice among providers or products, and • Support referrals and client and outcomes-based integration across programs, providers,  and services.   B. Too few asking the right questions and using the full scope of evidence to inform action. In too many federal programs, it is hard to find who is looking for and sharing information to inform where to focus, searching for positive outliers that might have replicable practices that would lead to better results in other places, running replication trials, convening conversations with the front line and others to learn from experience and well-designed trials, and communicating with those who want and need to know what has been learned both about what worked well and what did not, what might explain the differences, who will do what next, and why those next steps were chosen.    C. Siloed implementation organized around function and program, not outcomes and beneficiaries. Standard operating procedures such as budget processes and personnel rewards, as well as the risks associated with being perceived as operating too far outside your own lane instead of sticking to your knitting, make it hard for people in the federal government to manage to improve outcomes. This problem is exacerbated by information systems and data collection routines organized around programs rather than on outcomes and the people and places served.    Metzenbaum submission to the Advisory Committee on Data for Evidence Building, Feb. 9, 2021 13  Moreover, information collection protocols place more value on claiming burden reduction than on enhancing information value, treating data as the strategic value-adding asset it should be. New technologies make it more feasible than ever to organize information around outcomes and those served. It is time to update data standards, data systems, program guidance, and program practices around figuring out where to focus and finding ways to improve outcomes.    D. Insufficient attention to sharing evidence successfully with key audiences. Those working on the front line, whether in federal field offices or in the federal government’s purpose partners such as states, localities, tribes, territories, regions, and non-profits need knowledge that helps them decide where to focus (while also informing federal focus), find ways to improve, and increase uptake of better practices and reduced use of less good ones.  Communicating that knowledge requires not just finding and building it but also communicating it successfully so key audiences in the delivery chain can find and understand it and apply it appropriately. Some parts of the federal government already know how to do this well. Many others need to learn.    E. Oversight infrastructure that overwhelms insight generation and insight sharing. Finding the multiple organizational units in the federal government charged with oversight is not hard. They include but are not limited to GAO, Inspectors General, program monitors, and, for grant programs, the outside auditors every grant recipient getting more than $750,000 per year must bring in once a year to conduct an audit in accordance with the single uniform audit guidance.  These third-party observers collect a lot of data but seldom organize and analyze them in ways that suggest where to focus and how to improve. The Federal Emergency Management Administration has made noteworthy progress tapping technology and analytics to make its risk information about grants more useful. FEMA consolidated grant oversight information that had previously been collected by more than 40 grant programs into a single audit information repository, which includes information from previous GAO and IG reports as well as from Single Audits. Before this consolidation, FEMA looked at and dealt with oversight findings in a fragmented way. The IG would look at about 25 grantees per year and FEMA staff would work through the issues identified for each grantee individually. FEMA now tries to see and manage the forest as well as the trees to reduce adverse findings and reduce the need to de-obligate or recoup funding, which it is doing successfully.5 Its work is made much harder, though, because the Single Audits submitted to the Federal Audit Clearinghouse are submitted as images, not analyzable data. FEMA has to convert that information manually. Also, neither the GAO nor IGs seem to organize the information they collect in an analyzable way that makes it possible to see outcome or risk trends and patterns in ways that make it possible to learn across programs and time where to focus and how to improve.  The flip side of this problem is how hard it can be to find organizational units within the federal government looking for and sharing insights about what works and what works better. Some federal agencies, such as the National Highway Traffic Safety Administration and Ag Extension, have done this well for years. Others have made progress in recent years building evidence repositories but still need to make progress integrating evidence more seamlessly into program operations and helping the field find, build, and use evidence in addition to providing discrete funding awards to those ready to replicate successful past trials or scale them up in size. Moreover, as one long-time NHTSA expert reported to me, “Relatively little of our time is now devoted to consideration of what issues need attention and more to staying out of trouble…. The bureaucratic burdens of spending the grant money are about risk aversion.  There has been a   5 NAPA/Grant Thornton Grant Symposium meeting (https://www.napawash.org/uploads/FEMA_COD_Handout.pdf).    Metzenbaum submission to the Advisory Committee on Data for Evidence Building, Feb. 9, 2021 14  layering on of risk averse processes, procedures, checks, double checks so the money is spent on what we could easily do versus what needs to be done…. The Paperwork Reduction Act has been used as a tool to filter and enact administration policy…. OMB may cite methodology problems or research questions, but the way it feels is that they are trying to exert political influence if research is not going in the direction they like. This has been an increasing trend for several administrations… In my time at NHTSA, 20-30 people out of 50-60 on a team had Ph.D.’s. These people have been turned into contract administrators. They spend all of their time pursuing process layers of risk aversion. They got a Ph.D. in psychology and now spend all their time filling out forms. Risk aversion is not efficient. It feels like we are going in circles. We’ll ask, ‘Didn’t I just fill out that form?’  We get told, no, it’s different. But there is 80 percent overlap. The origin of these forms is not NHTSA, but the contract office. They don’t know why they have these forms and their bosses don’t know. They are caught in a tangled web of risk aversion.  It is not a rewarding environment for a scientist.”  F. Incentive systems that ignore evidence about motivational mechanisms. Many and possibly most people working on public policy problems in state, local, and other governments and in non-profits are motivated more by mission than money. Yet, too often, we link data to incentives for motivational purposes in ways that end up backfiring rather than in ways that use evidence about what actually motivates people. That evidence finds that doing well and getting specific useful feedback is motivating. It finds that setting a few stretch targets focuses, inspires, encourages persistence, and stimulates discovery if stretch targets are not too numerous nor overly ambitious relative to resources and skills and everyone appreciates that only a small number of stretch targets will actually be met (and if they were, they were not sufficiently stretch) but most stretch targets will lead to noteworthy performance gains.    G. Weak systems for learning and building knowledge across programs about what works well and what works less well for building, sharing, and using evidence. With a few noteworthy exceptions such as perhaps research and development and the Federal Demonstration Project, the federal government lacks strong mechanisms for collaborating and learning across policy areas and programs. For example, many federal programs try to prevent bad things from happening and keep their consequences as low as possible when they do. In other words, their mission is real-world and not just operational risk management. Other programs process benefits, process loans and regulate, some directly and some through other levels of government. Programs with similar types of goals and processes can (and have in the past) learned a lot from each other and collaborated for improvement. The Clinton Administration encouraged this kind of work through the National Performance Review, where communities of practice formed around different program types. The Bush Administration facilitated cross-program learning by categorizing program type for every program reviewed using the Program Assessment Rating Tool (PART). While PART and the website used to report PART scores had other problems including insufficient attention to outcomes progress and lessons learned instead of two program use of prescribed practices, its “program type” categorization was a step in the right direction. Technology advances that have driven down the cost of collecting, integrating, and analyzing data and driven up the ease of doing those things argue for coding far more of the data federal agencies collect in ways that not only identify outcomes to support collaboration to improve them but also to note goal type to facilitate learning and collaboration across programs with similar goal types about effective measurement methods and change mechanisms.       Metzenbaum submission to the Advisory Committee on Data for Evidence Building, Feb. 9, 2021 15  2. What are examples of high-impact data uses for evidence-based policy making that successfully effected change, reduced costs, or improved the welfare of citizens?   High-impact data uses include cleaner air, safer travel, healthier people, and better agricultural practices. More examples are provided below in the discussion of question 3 about frameworks, policies, and practices.  Let me also urge you to look for, document, and learn from low-impact data uses – examples where data have been collected but in a way that made the data hard to analyze to figure out where to focus and find ways to improve, including lower cost ways to achieve the same or better results. The Safe Drinking Water Act and the way it collects, analyzes, and shares analyses of Citizen Confidence Reports is arguably one example of low-impact data uses that perhaps partially explains government’s long delay responding to Flint, Michigan’s unsafe switch to unsafe water. Data submitted for review under the National Environmental Protection Act may be another example. What happens to all that information? Does it go into an analyzable data base somewhere or just stay in PDF or hard copy reports on some shelf or perhap get tossed? The Federal Audit Clearinghouse is another example, one that FEMA is happily suggesting how to turn into high-impact data. Education data may be another example of low-impact data uses:   The U.S. Education Department (ED) has supported numerous efforts over the years to look for positive outliers. Unfortunately, it is hard to find information about how ED is using those analyses to find ways to improve, test replicability of the most promising practices, explore applicability to different situations, and increase adoption of better practices. It is far easier to find information about how ED identified and punished low performers that did not make Adequate Yearly Progress. Published remarks by the head of the Institute for Education Sciences suggest that ED tried to replicate the success of the University of Chicago Consortium on School Research,6 a pre-cursor researcher-practitioner partnership believed to have contributed to the higher learning growth rate of the Chicago Public Schools that Stanford’s Education Opportunity Project found. Learning about those replication efforts and what was learned from them is not, however, easy. The IES Leader’s published remarks link to information about the Chicago Consortium but not to information about failed replication efforts from which others could learn.   Consider, too, that ED has published The Nation’s Report Card annually since 1990. According to one analysis, the most recent report suggests some, albeit not significant, progress in most but not all educational areas since the Report Card started with declining progress in most areas since 2017 and little improvement since 2009.7 Mississippi was the only state in the nation to show significant increases in three of the four core subjects measured in 2019, an improvement trajectory it has apparently been on since 2005.8 Washington, D.C. was the only jurisdiction to show gains in three of four subjects. At the same, time, national scores for most subjects dropped or remained flat from 2017 to 2019. What is unclear to this reader (and my guess is for many of those working on the front line who would like to know this information but don’t have time to search for it) is what these communities did to make a difference. ED has made clear through its tiered evidence grant-funding initiative, as well as its earlier financial support for Education Trust’s Dispelling the Myth project and its more recent financial support for the Stanford   6 https://ies.ed.gov/director/remarks/2-4-2020.asp 7 Nation’s Report Card. “Data Tools: State Profiles.” 2020. https://www.nationsreportcard.gov/profiles/stateprofile?chort=3&sub=MAT&sj=AL&sfj=NP&st=MN&year=2013R3 Accessed August 2020. See, also, Barshay, Jill. 2019. “U.S. education achievement slides backwards; Substantial decrease in reading scores among the nation's eighth graders.” The Hechinger Report, October 30, 2019. https://hechingerreport.org/u-s-education-achievement-slides-backwards/ 8 https://hechingerreport.org/opinion-four-ways-that-mississippi-is-teaching-more-children-to-read-well/    Metzenbaum submission to the Advisory Committee on Data for Evidence Building, Feb. 9, 2021 16  Education Opportunity Project, that it is eager to support others to search for effective practices, test replicability, and roll out the most promising. What is harder to find, if it exists somewhere, is a coherent and continually updated story that helps purpose partners see how all the pieces fit together – what ED has learned, what is currently being tried, and thoughts about future plans to try and to learn – so purpose partners can figure out how best to contribute. Some but not all of this story is in ED’s strategic and annual plans and its annual performance reports, and perhaps in its What Works Clearinghouse. It is not, however, communicated in an easily understood way to give purpose partners a coherent and up-to-date sense of how all the pieces fit together. It would be nice to see it more coherently laid out in ED’s strategic and annual plans, its performance report, its learning agenda, and in ways that current and potential purpose partners know about and find useful.     3. Which frameworks, policies, practices, or methods show promise overcoming challenges  experience by governments in their evidence-building?  As noted above, the most promising frameworks, policies, practices, and methods share several characteristics. They:   • are clear about the outcomes they seek to improve (although outcome objectives may change over time) and find ways to measure progress, problems, and context to inform where to focus and find ways to do better;  • communicate analyzed data and the findings of well-designed trials to current and often potential delivery partners in and outside the federal government in appropriately frequent, timely, understandable, easily accessible, and useful ways;  • embrace routines that discuss data analyses and findings of well-designed trials with those involved in delivery to figure out what has been learned, understand risks and causal factors that need attention, and decide what to do and what to learn next.  • communicate to key authorizers, other interested parties, and the public in easily understood and resonant language about outcome goals and why they were chosen, strategies and why they were chosen, progress, problem, lessons learned, and planned next steps both short and long term.     Here are some examples:  National Highway Traffic Safety Administration. The National Highway Traffic Safety Administration uses the Haddon matrix (named after the first director of the federal highway safety office who helped craft the law as a Congressional staffer) to measure traffic fatalities. All states measure and submit information to NHTSA for every traffic fatality about operator, equipment, physical characteristics, and the socio/economic characteristics before, during, and after each fatal accident. Some states also use the Haddon matrix to capture information about non-fatal accidents, which they share with the federal government.  Haddon’s measurement approach was informed by prior academic work on the epidemiology of accidents.9 Because it is so useful, this measurement method, still in use today, has informed the development of goals and measurement for other kinds of transportation accidents10 as well as for harmful incident measurement in other fields.    9 https://academic.oup.com/epirev/article/25/1/60/718691. Carol W. Runyan, Introduction: Back to the Future—Revisiting Haddon’s Conceptualization of Injury Epidemiology and Prevention, Epidemiologic Reviews, Volume 25, Issue 1, 1 August 2003, Pages 60–64, https://doi.org/10.1093/epirev/mxg005 10 Other federal transportation agencies use it, as well. See, for example, https://safety.fhwa.dot.gov/hsip/resources/fhwasa09029/app_c.cfm    Metzenbaum submission to the Advisory Committee on Data for Evidence Building, Feb. 9, 2021 17  NHTSA has worked closely with states and localities over the years to find bright spots and test bright spot replicability. Years ago the state of North Carolina brought to NHTSA’s attention an effort to increase safety-belt use started in Canada. With assistance from the NHTSA Regional Administrator, North Carolina tried replicating the practice with good results. South Carolina lacked the primary enforcement legal authority both Canada and North Carolina had allowing police to stop and check for safety belt use, so worked with the NHTSA Regional Administrator to test and measure an adapted version of what North Carolina did. Reduced fatalities resulted, although not as much as in North Carolina.11 Based on this experience, NHTSA rolled out a very successful national Click It or Ticket campaign to increase safety belt use in cars nationwide, pairing social marketing materials it provided to states, school officials, and others, allowing states and localities to adapt the material to local circumstances as appropriate. It also encouraged but did not mandate states to adopt primary enforcement laws. NHTSA built on this successful practice and applied it to its efforts to reduce distracted driving, testing a “Phone In One Hand, Ticket in the Other,” campaign in two smaller communities and after seeing outcome progress in those communities compared to other communities with similar characteristics, testing it in 4 larger communities. Over time, NHTSA has seen noteworthy progress on safety belt use,12 but growing problems with distracted driving.13 It has since tested several other campaigns to reduce distracted driving, including “One Text or Call Could Wreck It All,” and “U Drive, U Text, U Pay.”   Community Colleges and Networked Improvement Communities    The Carnegie Math Pathways NIC is a networked improvement community (NIC) dedicated to:   improving community college student success rates in developmental math courses by combining research-based knowledge, feedback from extensive conversations with key stakeholders, and the NIC’s own on-the-ground investigations of the experiences of actual community college students. The NIC developed a theory of improvement that included high-leverage drivers such as addressing language and literacy barriers and embedding supports for core math skill development within the curriculum to increase the proportion of students achieving college math credit in one year of continuous enrollment from 5% to 50%.14 As of September 2020, more than 40,000 students across 21 state had gone on to complete their introductory college math requirements at triple the rate of their peers, and transferred to and graduated from four-year colleges at significantly higher rates than their peers.15     According to one of the experts who launched this effort:   Key to reducing disparities in educational outcomes is a shift, a shift from a program focus (we need to add something new, some new idea or service) to a problem-solving focus (we target a specific disparity in outcomes and we keep iterating through improvement research cycles until we achieve our aim).    Now, being user-centered plays a critical role here. In retrospect, perhaps we could have found a better term for this second aspect of the first improvement principle. Our design school colleagues might prefer, for example, “human centered.” Regardless of term, the key idea remains: as you focus in on trying to address an educational inequity, bring the voices of the   11 Metzenbaum, “Strategies for Using State Information,” p. 32-33. 12 https://crashstats.nhtsa.dot.gov/Api/Public/ViewPublication/812875 13 https://crashstats.nhtsa.dot.gov/Api/Public/ViewPublication/812665 14https://www.carnegiefoundation.org/blog/five-essential-building-blocks-for-a-successful-networked-improvement-community/ 15 https://carnegiemathpathways.org/. Accessed September 17, 2020    Metzenbaum submission to the Advisory Committee on Data for Evidence Building, Feb. 9, 2021 18  people who are most directly impacted into the conversation. Seek to understand the dynamics of this disparity through the eyes, mindsets, and emotions of all involved.”16   New York City Building Inspections   Big data correlations combined with predictive modeling and continuing user feedback from the front-line helped design a measured trial for risk-based building inspections. A New York City data analytics team pulled together and analyzed data from 20 city data bases informed by frequent conversations with front-line building inspectors to identify warning signs correlated with unsafe buildings. Using that analysis, they proposed new priorities for scheduling building inspections which the building inspectors then tried. This resulted in inspectors finding conditions serious enough to warrant a vacate orders for 70 percent of inspected buildings compared to 13 percent when the prior scheduling method was used.17  Recovery Act Implementation Office  Implementation of the American Recovery and Reinvestment Act of 2009 showed how successful transparency requires identifying the people who need to be informed and involved in using information (both grantees and those they affect), aligning incentives to encourage their involvement, and supporting continuous learning and improvement communities (catalyzing their creation, where necessary.)   The experience of the Recovery Implementation Office (RIO) suggests the kind of mindset change grant programs, OMB, and oversight operations need to adopt to guide progress on a grant program’s outcome objectives while also wisely managing risks. The RIO employed a facilitative approach, while also leveraging the authority of the Vice President to facilitate the participation of stakeholders. The office functioned as a convener and problem-solver that engaged with a wide range of federal, state and local partners. This approach was embodied in the objectives identified by the Vice President when the office was established. These objectives included the expectation that office staff respond to requests and questions within 24 hours, cut across bureaucratic silos by reaching out to a variety of partners, and always be accessible. Toward this end, the office adopted the role of an “outcome broker,” working closely with partners across organizational silos at all levels of government in order to foster implementation of the Recovery Act and achieve results. Another role of the Recovery Implementation Office was to closely monitor Recovery Act spending. One way it did so was to monitor grants to ensure that they were consistent with the objectives identified by the Vice President. A second way the office monitored spending was to review weekly financial reports on agency obligations and expenditures for programs receiving Recovery Act funds and to meet with the agencies on a regular basis.  OMB sought to facilitate effective implementation of the Recovery Act by working to establish and strengthen relationships with state and local governments that would ultimately implement the programs on the ground. This was done in two ways: (1) by soliciting feedback from state and local partners when formulating and revising rules and policies governing the implementation of Recovery Act programs and (2) by developing its capacity to respond to questions from the many states and localities that would be implementing those rules   16 https://www.carnegiefoundation.org/wp-content/uploads/2017/04/Carnegie_Bryk_Summit_2017_Keynote.pdf, pp. 5-6. 17 https://slate.com/technology/2013/03/big-data-excerpt-how-mike-flowers-revolutionized-new-yorks-building-inspections.html.  Excerpt  from Viktor Mayer-Schönberger and Kenneth Cukier’s Big Data: A Revolution That Will Transform How We Live, Work, and Think, (Houghton Mifflin Harcourt: 2013.) (https://www.google.com/books/edition/Big_Data/HpHcGAkFEjkC?hl=en&gbpv=1&printsec=frontcover    Metzenbaum submission to the Advisory Committee on Data for Evidence Building, Feb. 9, 2021 19  and policies. A senior OMB official directly involved in this work told us the office had to move out of its traditional role as mainly a policy-making organization to adopt a more interactive and service-oriented approach. Under this approach, key activities involved engaging with and obtaining feedback from states and localities as well as providing technical support to these groups so that they could meet the Recovery Act’s numerous reporting requirements. For example, to obtain feedback from state and local partners when developing key Recovery Act policies, OMB became actively involved in weekly conference calls that included a diverse group of federal, state, and local organizations. Starting in the spring of 2009, regular participants in these calls included OMB; GAO; the National Association of State Auditors, Comptrollers and Treasurers; the National Governors’ Association; the National Association of State Budget Officers; the Recovery Board; the National Association of Counties; the National Association of State Chief Information Officers; and the National Association of State Purchasing Officers. These weekly calls were scheduled after several of these organizations wrote to OMB and GAO to express their strong interest in coordinating on reporting and compliance aspects of the Recovery Act. An important outcome of this regular information exchange was to make OMB aware of the need to clarify certain reporting requirements…. The local partners participating in these calls were able to corroborate what we reported and provide OMB with specific information about what additional guidance was needed. To obtain information to further guide refinements to the Recovery implementation process, at the end of 2009, OMB officials said they (1) interviewed and surveyed numerous stakeholders including governors and state and local recipients, and (2) worked with GAO to identify best practices. Based on these efforts, OMB subsequently revised its guidance, which focused on lessons learned around enhancing recipient reporting and compliance.   To improve technical support provided to state and local governments implementing the Recovery Act, OMB worked with the [oversight] Recovery Board to establish an assistance center based on an “incident command” model. One OMB official likened this approach to an extension of a traditional response model used during natural disasters, where the country’s economic condition during the Great Recession was the “incident” and the Recovery Act was the intervention to be rolled out through many partners. To help implement this approach, OMB worked with officials from the Department of Agriculture who offered the services of one of their national emergency management teams to help set up and coordinate this effort. Given the large number of state and local governments that needed to be supported, OMB requested that each agency with grant programs receiving Recovery Act funds contribute personnel to support the center. According to OMB officials, from September to mid-December of 2009, the center responded to approximately 35,000 questions from states and localities.18 (Bold, underlining, and words in brackets added.)   Effective communication to improve outcomes and operational quality needs to be frequent. It needs to be back-and-forth and inclusive – providing fast feedback while also informing longer-term strategic thinking. It needs to support brainstorming across grantees, continuous learning from analyzed data and tested theories of change, and appropriate application of knowledge from the field and the lab. The Recovery Act was exceptional in some ways because of the Vice President’s leadership role. In truth, though, every federal program needs a leader, appointed or career, to lead program implementation to improve outcomes working on their own and with their purpose partners across the federal government.   These are not the only examples of frameworks, policies, practices, or methods that have shown promise overcoming challenges experience by governments in their evidence-building. HHS’s HealthyPeople.gov   18 https://www.gao.gov/assets/670/660353.pdf . See, also,  https://obamawhitehouse.archives.gov/sites/default/files/new_way_of_doing_business.pdf     Metzenbaum submission to the Advisory Committee on Data for Evidence Building, Feb. 9, 2021 20  (2020), Partnership for Patients, and Winnable Battle campaigns are all promising, for example, and there is much to be learned from the experience of the USDA and the Veterans Benefits Administration’s use of data and well-designed trials.   The Government Performance and Results Act Modernization Act of 2010 essentially established and mandated the framework suggested here. Unlike its predecessor 1993 law, it established routines for priority setting within and across agencies and mandated more frequent discussion and communication of performance information pertaining to those priorities to figure out where to focus and, based on the evidence, decide what to do next. As one recent study by a former skeptic concluded:    Such routines made federal managers talk to each other about performance. In previous work, we also found that GPRAMA prodded managers pay more attention to program evaluations, an important concern given the implementation of the Evidence Act.19     At the same time, many still complain the GPRMA implementation is a compliance exercise. More work needs to be done to understand when it works well, when it does not, why, and how to make this frameworks work better.  Another recent study found real promise coupling experimental evaluation methods with principles of design-based implementation research (DBIR), improvement science (IS), and rapid-cycle evaluation (RCE) methods to provide relatively quick, low-cost, credible assessments of strategies designed to improve programs, policies, or practices.20  The challenge is to implement these frameworks in a way that accentuates the positive, as the Johnny Mercer song says, while addressing the negative. Technology advances not only make it easier to collect, analyze, and communicate data, data analyses, and the results of well-designed trials but also make it easier to support continuous learning and improvement communities with routines to ask the right questions and figure out who will do what next based on what was learned in the past.  My thanks to the Advisory Committee for asking these important questions. Don’t hesitate to contact me if you want me to elaborate on any of these thoughts and examples or point you to sources or if you have questions about confusing aspects of what is written here.      19 https://www.govexec.com/management/2021/02/ten-years-how-has-federal-performance-system-performed/171781/ 20 https://journals.sagepub.com/doi/10.1177/0193841X20923199     