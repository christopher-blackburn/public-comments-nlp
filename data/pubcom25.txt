 Comments for the Advisory Committee onData for Evidence BuildingFebruary 9, 2021 At Project Evident, we believe the practice of evaluation is overdue for transformation. More andmore, data and evidence are being used to inform funding decisions. But policymakers,evaluators, and practitioners often have different goals, and the dominant approach toevaluation is not always actionable: it remains slow, expensive, insufficient for decision making,and sporadic. As a result, the availability and usefulness of impact data and evidence variesgreatly. Project Evident was launched in 2017 out of a commitment to the building and use ofevidence and a deep dissatisfaction with the lack of improvement in community outcomes. Wesupport practitioners in taking leadership for their evidence building, and we help funders tosupport them in implementing evidence-building programs that focus on continuous learningand improvement. The COVID-19 pandemic and the Black Lives Matter movement have put a stark spotlight on theracial and economic inequities that stem from policies and practices in health, housing,education, employment, criminal justice, and other domains. There is immense urgency forsystemic reforms and effective solutions that deliver equitable outcomes for communities ofcolor and low-income communities. Federal, state, and local government agencies, along withorganizations that serve our communities, need high-quality data and evidence to address thesepressing challenges, and to identify cost-effective solutions that reduce persistence disparitiesin outcomes that are too often based on race, income, and geography. Despite progress made under both the Bush and Obama administrations to promote the use ofevidence, practitioners, communities, funders, and policymakers are not systematicallygenerating and using the evidence necessary to better serve disadvantaged communities. Wesee several pain points that are preventing the more widespread adoption of evidence-basedpractice, as well as opportunities for the Advisory Committee and the Biden administration –along with practitioners, funders, and researchers – to advance a stronger evidence ecosystemthrough supporting Strategic Evidence Planning for federal agencies, broadening definitions ofevidence, and increasing investments in our evidence infrastructure. We applaud the work the   Advisory Committee is undertaking and appreciate the opportunity to provide input. Our comment addresses questions 1 and 3. Support Effective Policymaking Through Strategic Evidence Planning Among the challenges that federal, state, and local governments face in using data and evidence to inform policy is the general absence of a roadmap to guide this work. The Evidence Act addresses this challenge in part by requiring many agencies to develop research and learning agendas. We support this requirement, and suggest implementing a more robust vision of this work in the form of Strategic Evidence Plans, which provide a roadmap for continuous evidence building and program improvement, and allow agencies to strengthen their culture of learning and innovation, optimize programs and practices, and scale proven solutions. Just as a strategic plan ensures that an organization’s decision-making is intentional and is in service of measurable goals, a Strategic Evidence Plan (SEP) ensures that an organization’s efforts to use data for evaluating and improving its work are planful and can advance its vision for impact. The SEP is a roadmap that helps programs and organizations: ● Articulate a vision for how evidence can advance strategic priorities within a specifictimeline, and identify concrete goals to help achieve that vision; ● Develop a learning agenda that prioritizes critical gaps in knowledge and evidence insupport of the vision and goals; ● Identify investments and actions needed to achieve the vision and goals, based onassessments of operating context and institutional capacity (including tools, data,technology and talent); and ● Outline a sequence of actionable steps to implement the investments and actions. Project Evident pioneered the SEP process with nonprofit social sector organizations with therecognition that the field needed a new, strategic approach to continuous evidence building thatgoes beyond the one-study-at-a-time approach, and that advances actionable, practicalknowledge needed to build and scale solutions. SEPs are also designed to leverage andprioritize the voices of practitioners and public administrators who are closest to theimplementation of programs and policies in the communities, making the process of buildingevidence more equitable than it traditionally has been. We’ve since worked with many different types of public and private organizations, includinggovernment agencies at the state and local level (like local education authorities), to developSEPs that have empowered practitioners, policymakers and administrators to accelerateinvestments in R&D infrastructure and practices, and build the evidence that they need to 2   improve outcomes for their communities. We’ve learned that SEPs can be customized to benefitorganizations of different sizes and types, as they are grounded in each agency’s vision forimpact, evidence goals, and operational reality. The final report from the bipartisan Commission on Evidence-Based Policymaking found thatfederal departments “frequently do not have an integrated approach or a long-range plan forevidence building” and that OMB’s existing infrastructure and organizational capacity “does notoptimize the agency’s ability to coordinate evidence building across the federal government.”The Commission recognized the importance of improving the institutional capacity and cultureof federal agencies to use data and evidence in policymaking, and emphasized that agencies“must be empowered and organized to work together and accomplish shared goals.” Inresponse to the Commission’s findings, the 2019 Evidence Act mandates that every agencydevelop a multi-year learning agenda that outlines evidence priorities, and an annual evaluationplan describing significant evaluation undertakings. The legislation also asks agencies toconduct “capacity assessments for research for statistics, evaluation, research and analysis” asa part of their strategic planning process every four years. All of these pieces — learning agendas, capacity assessments and evaluation plans — areintegral components of the SEP process. The key differences are that they are connected to aconcrete vision for impact and time-bound evidence goals, and are developed as part of aseamless progression to ensure coherent implementation. For example, we work to ensure thatlearning agendas for our partners are closely aligned with their goals for evidence building. Our“context and capacity review” process takes a holistic approach to assessing an organization’sability to execute on their evidence goals, including staffing, technology, partnerships, andculture of data use and learning — leading to a clear roadmap that outlines the investments andactivities for capacity building and continuous evidence generation. As such, SEPs can be a vitaltool to support evidence-based policymaking across government agencies. At the same time, asdiscussed below and on Project Evident’s blog, we also encourage federal agencies to provideresources to government agencies and service providers at the state and local level to developtheir own SEPs. Broaden Definitions of Evidence To build an evidence ecosystem that is more timely and cost-effective, we need to broaden howwe define evidence. We have relied too heavily on frameworks and definitions that are overlynarrow and don’t promote continuous evidence building, but rather contribute to a “thumbs up orthumbs down” or “one and done” mentality. We have falsely equated rigor with RCTs alone, and 3   paid too little attention to the equally important evidence building at the earlier stages. And wegenerate evidence that is not always relevant, timely, or financially viable. For instance, the tiered evidence framework in the Every Student Succeeds Act (ESSA) creates anumber of issues for districts, schools, and education nonprofits, including prioritizing methodslike RCTs over other approaches including descriptive statistics; an overemphasis on statisticalsignificance; and inadequate consideration of cost. Because the use of descriptive statistics isnot included in ESSA’s four evidentiary tiers, this practical approach is not incentivized and, as aresult, is more likely to be overlooked as a strategy to improve school outcomes. Another issue is that we risk overlooking valuable learnings when we discount evidence thatdoesn’t meet certain criteria, as happened with a large national nonprofit that underwent arigorous multi-study evaluation in 2006. During that time, the What Works Clearinghousechanged its evidence standards with regard to effect size. As a result, the entire $5m, 5-yearstudy was largely discounted by key stakeholders and key information was generallyoverlooked – including a small scale RCT that showed that the studied approach improvedgraduation rates for Black male students in Texas. Without addressing the merits and/ordrawbacks of the change in effect size, we would encourage adoption of a more inclusiveframework for interpreting results, using less of a binary (thumbs up thumbs down) approachwhere a program either works or it doesn’t. In this case, there was useful learning to be gainedfrom the rigorous evaluation even though it had low effect sizes, especially since we lackenough programs that meet the highest standards. We should instead take a broader approach to evidence building, aimed at addressing prioritiesand in a way that is more actionable – leveraging readily accessible forms of data and factoringin considerations of implementation context, data quality, and systemic drivers of inequities. Forinstance, after closing more than 20 schools, an urban school district revised its processesusing available enrollment, utilization, budget, and school selection data. As a result of theseevidence-informed improvements, more than 8,000 students were placed in better performingschools. This timely, low-cost approach to improving outcomes would not typically “count” asevidence within traditional frameworks, researchers or funders, but should be supportednonetheless. A more balanced ratio of summative evaluations to practitioner-led strategic evidence buildingcan help spur innovation and real-time evidence that is urgently needed as organizations figureout how best to serve the needs of their communities during and after the pandemic. At ProjectEvident, we recently experienced a recent bright spot with a funder that was pushing for an RCTof its program in hopes of securing the strongest assessment of impact, despite the fact thatconditions on the ground could not support one – leaving the future of a highly promising 4   program uncertain. After significant analysis, we were able to work with the funder to pursue astaged approach to developing evidence that can help them optimize their model for impact andsustainability, and that takes into account community context, partnerships, and cost. Withoutthis work, the program would not have had the opportunity to improve services forresource-constrained communities. Increase Investments in R&D Infrastructure and Practices A major barrier to evidence-based policymaking at scale is that the demand for effective socialprograms aimed at addressing persistent and pressing problems far outweighs the supply. Thecapacity for evidence building in the social and education sectors is highly varied, ranging frommainly rudimentary infrastructure for data collection and analysis and limited dollars for talentacquisition and development in the nonprofit sector, to a robust technical data collectionapparatus among public education agencies. Yet government and philanthropic funders rarelyhelp nonprofit and public practitioners build core capacity for data collection, evidence building,and evidence-informed continuous improvement. Data collection and reporting requirementsare often centered around compliance, and do not incentivize R&D or learning. In order to spur innovation and continuous improvement in the social and education sectors, adisciplined process for learning, testing, and improving – an ‘R&D approach’ – must becomestandard and supported practice. In 2019 five major foundations (the Ford, Hewlett, MacArthur,Open Society and Packard foundations) announced that they were shifting their fundingstrategies to support nonprofit administrative costs essential to achieving impact, includinginformation technology, strategic planning, and knowledge management, and embarking on acampaign to encourage other funders to do the same. We applaud this movement andencourage similar trends in government funding policies to better support and promotepractitioners’ learning, implementation, and organizational development. For instance, buildingon the Evidence Act, OMB should broaden the list of evidence-building activities allowable underfederal grants, as Project Evident and a coalition of nonprofit and government leaders haverecommended. In addition to increasing the amount of funding for evidence capacity, we need to shift thepredominant focus of evaluation from accountability to learning – a sentiment we hear fromboth practitioners and researchers. This means balancing existing support for compliancestudies and point-in-time impact evaluation with increased investment in building practitioner’scapacity to generate and use evidence for learning and improvement. We should also incentivizeevaluators, researchers, and other impact intermediaries to partner with practitioners to figureout what they need to know in order to better understand who they are serving, how well they are 5   serving them, and what impact they are having. And in order to truly improve, we need to makeroom for risk, failure, and adjustment. Agencies should also invest in their own capacity to learn, test, and improve. We recognize thatnot all actions can be traditionally tested, but encourage testing where feasible. We like theapproach laid out in the most recent U.S. Securities and Exchange Commission Annual Report,in the chapter on POSITIER (beginning on page 51), the SEC’s investor testing initiative whichoffers steps and principles for evidence-based policymakers. Brian Scholl, Principal EconomicAdvisor and Chief Architect of POSITIER, stresses the importance of becoming a learningorganization, which requires regular evaluation and assessment, as well the acceptance ofunfavorable results. We would also point out that strong leadership is key to building the type ofcollaborative work environment that welcomes intellectual diversity and challengingassumptions, and allows for going back to the drawing board when necessary. And funding isneeded to train agency leadership to better use and learn from data. As we have seen in our experiences working with hundreds of nonprofits and agencies,becoming a data-driven organization is not easy. A recent article from Harvard Business Reviewnotes that while private sector companies have made progress with appointing chief dataofficers, they still struggle to create strong data cultures, and report year after year that “culturalchallenges — not technological ones — represent the biggest impediment around datainitiatives.” We understand that agencies face the same challenges, and above all, we want toavoid compliance with the letter versus the spirit of the Evidence Act. As Katharine Abrahamnoted recently during a webinar marking the Evidence Act’s 2-year anniversary, "The success ofthe Evidence Act requires not just written rules in place, but having leaders that truly believe init.” To that end, adopting the use of strategic evidence plans can help make requirements suchas learning agendas and capacity assessments more meaningful for agencies. Broadening howwe define evidence can make data-driven practice more widespread and accessible, rather thansomething thought of as only relevant to large, sophisticated organizations. And increasinginvestment in both human and technical R&D infrastructure and practices will enablegovernment and social sector leaders to drive their own agencies and organizations in learning,testing, and improving their impact. 6 